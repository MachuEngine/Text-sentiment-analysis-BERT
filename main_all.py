# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
import matplotlib.pyplot as plt
from datasets import load_dataset_builder
from datasets import load_dataset

"""
Hugging Faceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ BERT ëª¨ë¸ì„ í™œìš©í•œ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰
- `BertTokenizer: BERT ëª¨ë¸ì— ë§ê²Œ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ëŠ” ë„êµ¬
- `get_linear_schedule_with_warmup`: í•™ìŠµë¥ ì„ ì¡°ì ˆí•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬
- `AdamW`: Adam ì˜µí‹°ë§ˆì´ì €ì˜ ë³€í˜•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê°ì‡ (weight decay)ë¥¼ ì ìš©í•˜ëŠ” ì˜µí‹°ë§ˆì´ì €
- `BertForSequenceClassification`: ê°ì • ë¶„ì„ ë“± ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•œ BERT ëª¨ë¸
"""

# 1. ë°ì´í„°ì…‹ ëª¨ë“ˆ: í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì„ ë°›ì•„ BERT ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
class SentimentDataset(Dataset):

    """
    *. __init__(): ë°ì´í„°ì…‹ì„ ì´ˆê¸°í™” 
    - texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    - labels: ê° í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ê°ì • ë ˆì´ë¸” (ì˜ˆ: ê¸ì •/ë¶€ì •)
    - tokenizer: í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•  ë•Œ ì‚¬ìš©í•  BERT í† í¬ë‚˜ì´ì €
    - max_len: í† í° ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •

    *. __getitem__(): ì£¼ì–´ì§„ ì¸ë±ìŠ¤(idx)ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì„ ê°€ì ¸ì™€ BERT ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    encoding = tokenizer.encode_plus(
    : í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  BERTê°€ ìš”êµ¬í•˜ëŠ”ëŠ” ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜

    - add_special_tokens=True: [CLS]ë‚˜ [SEP] ê°™ì€ íŠ¹ë³„ í† í° ì¶”ê°€
    - max_length: ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •í•˜ì—¬ ê¸¸ì´ë¥¼ ê³ ì •
    - padding='max_length': ê¸¸ì´ê°€ ë¶€ì¡±í•  ê²½ìš° íŒ¨ë”©ì„ ì¶”ê°€
    - truncation=True: ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
    - return_attention_mask=True: íŒ¨ë”© ë¶€ë¶„ì„ ë§ˆìŠ¤í‚¹í•˜ëŠ” ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
    - return_tensors='pt': ê²°ê³¼ë¥¼ PyTorch í…ì„œë¡œ ë°˜í™˜

    *. attention_maskê°€ í•„ìš”í•œ ì´ìœ 
    - BERT ëª¨ë¸ì€ **ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ê³ ì •(max_length)**í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ê¸¸ì´ê°€ ì§§ì€ ë¬¸ì¥ì€ ë¹ˆ ê³µê°„ì„ [PAD] í† í°(=0)ìœ¼ë¡œ ì±„ì›Œì•¼ í•¨.
      í•˜ì§€ë§Œ ëª¨ë¸ì´ íŒ¨ë”©ì„ ì˜ë¯¸ ì—†ëŠ” ê°’ìœ¼ë¡œ ì¸ì‹í•˜ê³  ë¬´ì‹œí•´ì•¼ í•¨ 
      â†’ ì´ë¥¼ ìœ„í•œ ê²ƒì´ attention_maskì´ë‹¤.

       attention_maskì˜ ì—­í• 
        attention_maskëŠ” BERT ëª¨ë¸ì´ ì…ë ¥ ë¬¸ì¥ì—ì„œ ì‹¤ì œ ë‹¨ì–´ì™€ íŒ¨ë”©(PAD)ì„ êµ¬ë³„í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì—­í• ì„ í•œë‹¤.
        ì¦‰, BERTê°€ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë§Œ í•™ìŠµí•˜ê³ , íŒ¨ë”©ëœ ë¶€ë¶„ì„ ë¬´ì‹œí•˜ë„ë¡ ìœ ë„í•˜ëŠ” ì—­í• ì„ í•œë‹¤.

    *. return 
    ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    â†’ ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ì…ë ¥ ë°ì´í„°(í† í° ID, ì–´í…ì…˜ ë§ˆìŠ¤í¬)ì™€ ì›ë³¸ í…ìŠ¤íŠ¸, ë ˆì´ë¸”ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
    .flatten()ì€ í…ì„œì˜ ì°¨ì›ì„ í‰í‰í•˜ê²Œ ë§Œë“¤ì–´ 1ì°¨ì› ë°°ì—´ë¡œ ë§Œë“ ë‹¤.
    """
    # í´ë˜ìŠ¤ ì •ì˜ ë° ì´ˆê¸°í™”
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    # ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œ ê°œìˆ˜ ë°˜í™˜
    def __len__(self):
        return len(self.texts)

    #  ì£¼ì–´ì§„ ì¸ë±ìŠ¤(idx)ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì„ ê°€ì ¸ì™€ BERT ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    def __getitem__(self, idx):
        text = str(self.texts[idx])# self.texts[idx]ê°€ strì¼ ìˆ˜ë„ ìˆì§€ë§Œ í˜¹ì‹œ ëª¨ë¥´ë‹ˆ strë¡œ ë³€í™˜
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 2. í•™ìŠµ ëª¨ë“ˆ: ì—í¬í¬ ë‹¨ìœ„ë¡œ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
def train_epoch(model, data_loader, optimizer, device, scheduler):

    """ 
    data_loaderë¡œë¶€í„° batchë¥¼ í•˜ë‚˜ì”© ê°€ì ¸ì™€ ëª¨ë¸ì„ í•™ìŠµ
    bata_loaderëŠ” train_datasetì„ DataLoaderë¡œ ë³€í™˜í•œ ê²ƒ

    batchì—ì„œ input_ids ì—´ ì¶”ì¶œí•˜ì—¬ deviceë¡œ ì´ë™
    attention_mask ì—´ ì¶”ì¶œí•˜ì—¬ deviceë¡œ ì´ë™
    labels ì—´ ì¶”ì¶œí•˜ì—¬ deviceë¡œ ì´ë™

    modelì— input_ids, attention_mask, labelsë¥¼ ì…ë ¥í•˜ê³  ì¶œë ¥ì„ ë°›ìŒ
    ì¶œë ¥ìœ¼ë¡œë¶€í„° ì†ì‹¤ê°’ì„ ê³„ì‚°í•˜ê³  total_lossì— ë”í•¨
    ì†ì‹¤ê°’ìœ¼ë¡œ ì—­ì „íŒŒ ìˆ˜í–‰í•˜ì—¬ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ í•™ìŠµë¥  ì¡°ì ˆí•˜ë©°, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” 
    ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”í•˜ëŠ” ì´ìœ  : ê° ë°°ì¹˜ë§ˆë‹¤ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•¨. 
    
    (ì¤‘ìš” : ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì„ !!!)

    ğŸ”¹ 1. loss.backward()ê°€ ë¬´ì—‡ì„ í•˜ëŠ”ê°€?
      loss.backward()
      PyTorchì—ì„œëŠ” ì—­ì „íŒŒ(Backpropagation) ë¥¼ ìˆ˜í–‰í•˜ë©´,
      ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜)ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸(gradient)ê°€ ëˆ„ì ë¨.
      ì¦‰, loss.backward()ë¥¼ í˜¸ì¶œí•˜ë©´ í˜„ì¬ ë°°ì¹˜(batch)ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê³„ì‚°ë˜ì–´ ëª¨ë¸ì˜ grad ì†ì„±ì— ì €ì¥ë¨.

    ğŸ”¹ 2. optimizer.step()ëŠ” ë¬´ì—‡ì„ í•˜ëŠ”ê°€?
      optimizer.step()ì€ í˜„ì¬ ë°°ì¹˜ì—ì„œ ê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì´ìš©í•´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸

    ğŸ”¹ 3. optimizer.zero_grad()ê°€ í•„ìš”í•œ ì´ìœ 
      PyTorchì˜ ê¸°ë³¸ ë™ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸(gradient)ê°€ ëˆ„ì ë˜ëŠ” ê²ƒ.
      ì¦‰, loss.backward()ë¥¼ í˜¸ì¶œí•  ë•Œë§ˆë‹¤ ì´ì „ ë°°ì¹˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ëˆ„ì ë˜ì–´ ì ì  ì»¤ì§.
      ì´ë¥¼ ë°©ì§€í•˜ë ¤ë©´ ë§¤ ë°°ì¹˜ë§ˆë‹¤ optimizer.zero_grad()ë¥¼ í˜¸ì¶œí•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•´ì•¼ í•¨.

      ë°°ì¹˜ë¥¼ ì—¬ëŸ¬ ë²ˆ ëŒë¦¬ëŠ” ì´ìœ ëŠ” ê°€ì¤‘ì¹˜ê°€ ì•„ë‹ˆë¼ ê·¸ë˜ë””ì–¸íŠ¸(Gradient) ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ì„œ.
      optimizer.zero_grad()ëŠ” ê°€ì¤‘ì¹˜(Weights)ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê·¸ë˜ë””ì–¸íŠ¸(Gradient)ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒ.
      ì¦‰, ê°€ì¤‘ì¹˜ëŠ” ê³„ì†í•´ì„œ ì—…ë°ì´íŠ¸ë˜ë©°, ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì´ˆê¸°í™”ë˜ëŠ” êµ¬ì¡°.
    """

    model.train()
    total_loss = 0

    for batch in data_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    return total_loss / len(data_loader)

# 3. í‰ê°€ ëª¨ë“ˆ: ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
def eval_model(model, data_loader, device):

    """ 
    ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜

    data_loaderë¡œë¶€í„° batchë¥¼ í•˜ë‚˜ì”© ê°€ì ¸ì™€ ëª¨ë¸ì„ í‰ê°€
    batchì—ì„œ input_ids ì—´ ì¶”ì¶œí•˜ì—¬ deviceë¡œ ì´ë™
    batchì—ì„œ attention_mask ì—´ ì¶”ì¶œí•˜ì—¬ deviceë¡œ ì´ë™
    batchì—ì„œ labels ì—´ ì¶”ì¶œí•˜ì—¬ deviceë¡œ ì´ë™
    modelì— input_ids, attention_maskì„ ì…ë ¥í•˜ê³  ì¶œë ¥ì„ ë°›ìŒ
    ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´ torch.no_grad()ë¡œ ê°ì‹¸ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”
    ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš© (ëª¨ë¸ì˜ ì¶œë ¥ì€ ë¡œì§“ ê°’ì´ë©°, ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤ê°€ ì˜ˆì¸¡ê°’)

    logit ì´ë€? 
    ë¡œì§“ì€ Softmax í•¨ìˆ˜ ì ìš© ì „ì˜ ì¶œë ¥ê°’ìœ¼ë¡œ, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê° í´ë˜ìŠ¤ì˜ ì ìˆ˜(score) ë¥¼ ì˜ë¯¸.

    âœ” outputs.logitsëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì›ì‹œ ì ìˆ˜(Softmax ì ìš© ì „)ì´ë©°, í™•ë¥ ì´ ì•„ë‹˜.
    âœ” torch.max(outputs.logits, dim=1)ë¥¼ ì‚¬ìš©í•˜ë©´ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥.
    âœ” í™•ë¥ ì´ í•„ìš”í•˜ë‹¤ë©´ Softmax(ë‹¤ì¤‘ ë¶„ë¥˜) ë˜ëŠ” Sigmoid(ì´ì§„ ë¶„ë¥˜)ë¥¼ ì ìš©í•´ì•¼ í•¨.

    ì¦‰, logitsëŠ” "ëª¨ë¸ì˜ ìµœì¢… ì˜ˆì¸¡ ì ìˆ˜"ì´ë©°, í™•ë¥ ì´ ì•„ë‹Œ ì›ì‹œ ê°’ì´ë¯€ë¡œ ì¶”ê°€ ë³€í™˜ì´ í•„ìš”í•  ìˆ˜ ìˆë‹¤ë‹¤!

    ì˜ˆì¸¡ê°’(logitì´ ê°€ì¥ í°ê°’ì˜ )ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ë¹„êµí•˜ì—¬ ì •í™•í•œ ì˜ˆì¸¡ ìˆ˜ë¥¼ ê³„ì‚°

    outputì€ ê°ì²´ì´ê¸° ë•Œë¬¸ì— .logitê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

    ë¦¬í„´ì€ ì •í™•íˆ ë§ì¶˜ ì˜ˆì¸¡ ìˆ˜ë¥¼ ì „ì²´ ë°ì´í„° ê°œìˆ˜ë¡œ ë‚˜ëˆ  í‰ê·  ì •í™•ë„ë¥¼ ê³„ì‚°
    """
    model.eval()
    correct_predictions = 0
    total = 0

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs.logits, dim=1)
            correct_predictions += torch.sum(preds == labels)
            total += labels.size(0)

    return correct_predictions.double() / total

# 4. ë°ì´í„°ì…‹ ìƒ˜í”Œ ì‹œê°í™”: ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œ êµ¬ì¡°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ 3ê°œë§Œ ì¶œë ¥ 
def visualize_dataset_sample(dataset, num_samples=16):
    """ 
    num_samplesë§Œí¼ ë°ì´í„°ì…‹ ìƒ˜í”Œì„ ì¶œë ¥
    datasetì€ train_datasetì´ë©°, train_datasetì€ SentimentDataset í´ë˜ìŠ¤ì˜ ê°ì²´ì„.
    datasetì˜ ië²ˆì§¸ ìƒ˜í”Œì„ ê°€ì ¸ì™€ text, input_ids, attention_mask, labels ë“±ì„ ì¶œë ¥
    """
    print("=== dataset sample ===")
    for i in range(num_samples):
        sample = dataset[i]
        print("=== ì¸ë±ìŠ¤i dataset ===")
        print(sample)
        print(f"sample {i+1}:")
        print("text:", sample['text'])
        print("input token IDs:", sample['input_ids'])
        print("attention mask:", sample['attention_mask'])
        print("label:", sample['labels'])
        print("-----")

# 5. ëª¨ë¸ ê²°ê³¼ ì‹œê°í™” (í…ìŠ¤íŠ¸ ì¶œë ¥): ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ì˜ˆì¸¡ëœ ê²°ê³¼, ì‹¤ì œ ë ˆì´ë¸” ë° ë¡œì§“ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
def visualize_model_results(model, data_loader, device, num_samples=1):
    model.eval()
    print("=== model result sample ===")
    with torch.no_grad():
        for i, batch in enumerate(data_loader):
            if i >= num_samples:
                break
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs.logits, dim=1)
            for j in range(len(batch['text'])):
                print(f"input text: {batch['text'][j]}")
                print(f"prediction: {preds[j].item()}, label: {labels[j].item()}")
                print(f"logit: {outputs.logits[j].cpu().numpy()}")
                print("-----")

# 6. ëª¨ë¸ ê²°ê³¼ ì‹œê°í™” (ê·¸ë˜í”„): Matplotlibì„ ì‚¬ìš©í•´ ê° í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
def visualize_model_results_with_plot(model, data_loader, device, num_samples=1):
    model.eval()
    print("=== ëª¨ë¸ ê²°ê³¼ ì‹œê°í™” (ê·¸ë˜í”„) ===")
    with torch.no_grad():
        for i, batch in enumerate(data_loader):
            if i >= num_samples:
                break
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            for j in range(len(batch['text'])):
                probs = torch.softmax(outputs.logits[j], dim=0).cpu().numpy()
                classes = [f"class {k}" for k in range(len(probs))]
                plt.figure(figsize=(6,4))
                plt.bar(classes, probs, color='skyblue')
                plt.title(f"text: {batch['text'][j][:30]}...\nlabel: {labels[j].item()}, prediction: {torch.argmax(outputs.logits[j]).item()}")
                plt.xlabel("class")
                plt.ylabel("probability")
                plt.ylim(0, 1)
                plt.show()

def main():

    """ 
    * Hyperparameters ì„¤ì •
    - PRE_TRAINED_MODEL_NAME: ì‚¬ì „ í›ˆë ¨ëœ BERT ëª¨ë¸ ì´ë¦„
    - MAX_LEN: í† í° ì‹œí€€ìŠ¤ì˜ ìµœëŒ€ ê¸¸ì´
    - BATCH_SIZE: ë°°ì¹˜ í¬ê¸°
    - EPOCHS: ì—í¬í¬ ìˆ˜
    - DEVICE: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (GPU ë˜ëŠ” CPU)

    * í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
    - tokenizer: tokenizerëŠ” Hugging Faceì˜ BertTokenizer í´ë˜ìŠ¤ ê°ì²´
    ğŸ”¹ tokenizer ê°ì²´ì˜ ì—­í• 
        - í…ìŠ¤íŠ¸ë¥¼ BERT ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹(í† í° ID)ë¡œ ë³€í™˜
        - BERTê°€ ìš”êµ¬í•˜ëŠ” íŠ¹ìˆ˜ í† í°([CLS], [SEP]) ì¶”ê°€
        - íŒ¨ë”©(Padding) ë° ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •
        - ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (íŒ¨ë”©ëœ ë¶€ë¶„ì„ ë¬´ì‹œí•˜ë„ë¡ ì„¤ì •)
    ì¦‰, tokenizerëŠ” í…ìŠ¤íŠ¸ë¥¼ BERT ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•¨. (ë¬¸ì¥ì„ ìˆ«ìë¡œ ë³€í™˜)

    - model: modelì€ Hugging Faceì˜ BertForSequenceClassification í´ë˜ìŠ¤ ê°ì²´
    ğŸ”¹ model ê°ì²´ì˜ ì—­í• 
        - BertModelì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¬¸ì¥ ë¶„ë¥˜ ëª¨ë¸
        - ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ ìœ„ì— ë¶„ë¥˜ìš© ì„ í˜•(Linear) ë ˆì´ì–´ ì¶”ê°€
        - ì…ë ¥ëœ ë¬¸ì¥ì˜ ê°ì • ë¶„ì„(ê¸ì •/ë¶€ì •) ìˆ˜í–‰
    ì¶œë ¥ : [1.23, -0.98]: ê¸ì •(1.23), ë¶€ì •(-0.98)ì˜ ë¡œì§“ ê°’
    ã„´ torch.argmax(outputs.logits, dim=1)ì„ í•˜ë©´ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.

    * IMDb ê°ì • ë¶„ì„ ë°ì´í„°ì…‹ ë¡œë“œ
    Hugging Faceì˜ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ IMDb ë°ì´í„°ì…‹ì„ ë¡œë“œ


    accuracy_tensor.item()ì˜ ì˜ë¯¸
    .item()ì„ í˜¸ì¶œí•˜ë©´ í…ì„œë¥¼ Pythonì˜ ìˆ«ì(float) ê°’ìœ¼ë¡œ ë³€í™˜.
    ã„´ .item()ì„ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ tensor(0.8923)ì²˜ëŸ¼ ì¶œë ¥ë¨.
    ã„´ .item()ì„ ì‚¬ìš©í•˜ë©´ 0.8923ì´ë¼ëŠ” ìˆœìˆ˜í•œ Python float ê°’ì´ ë¨.


    """
    
    # ----------------------------------------------------------------------------
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'
    MAX_LEN = 128
    BATCH_SIZE = 16
    EPOCHS = 3
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ----------------------------------------------------------------------------
    # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
    model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=2)
    model = model.to(DEVICE)

    # ----------------------------------------------------------------------------
    # IMDb ê°ì • ë¶„ì„ ë°ì´í„°ì…‹ ë¡œë“œ (í›ˆë ¨ + í…ŒìŠ¤íŠ¸ ë¶„ë¦¬)
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“°ì§€ ì•Šê³ , HTTP API ìš”ì²­ì„ í†µí•´ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ë°›ì„ ìˆ˜ë„ ìˆìŒ
    # ì—¬ê¸°ì„œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ë„ë¡ í•¨
    print("Downloading IMDb dataset...")
    dataset = load_dataset("imdb") # Hugging Faceì˜ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ APIë¥¼ í˜¸ì¶œ

    # í›ˆë ¨ ë°ì´í„° (25,000ê°œ ì¤‘ 5000ê°œë§Œ ì‚¬ìš©)
    # í›ˆë ¨ ë°ì´í„° 5000ê°œë¥¼ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œë§
    train_data = dataset["train"].shuffle(seed=42).select(range(150))
    train_texts = train_data["text"]
    train_labels = train_data["label"]

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° 500ê°œë¥¼ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œë§
    test_data = dataset["test"].shuffle(seed=42).select(range(10))
    test_texts = test_data["text"]
    test_labels = test_data["label"]

    print(f"Training data size: {len(train_texts)}")
    print(f"Test data size: {len(test_texts)}")

    # í›ˆë ¨ ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±
    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, MAX_LEN)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° DataLoader ìƒì„± (í›ˆë ¨ê³¼ ë³„ê°œë¡œ ìœ ì§€)
    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, MAX_LEN)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # --------------------------------------------------------------
    # ë°ì´í„°ì…‹ ìƒ˜í”Œ ì‹œê°í™” (í›ˆë ¨ ë°ì´í„°)
    visualize_dataset_sample(train_dataset, num_samples=3)

    #--------------------------------------------------------------
    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
    total_steps = len(train_loader) * EPOCHS
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    # --------------------------------------------------------------
    # ì—í¬í¬ ë‹¨ìœ„ë¡œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    for epoch in range(EPOCHS):
        print(f'\nEpoch {epoch+1}/{EPOCHS}')
        train_loss = train_epoch(model, train_loader, optimizer, DEVICE, scheduler)
        print(f'Train loss: {train_loss:.4f}')

        # í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ ì •í™•ë„ í‰ê°€ (ê³¼ì í•© ì—¬ë¶€ í™•ì¸)
        train_accuracy = eval_model(model, train_loader, DEVICE)
        print(f'Training Accuracy: {train_accuracy.item()*100:.2f}%')

        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì •í™•ë„ í‰ê°€ (ì‹¤ì œ ì„±ëŠ¥ í™•ì¸)
        test_accuracy = eval_model(model, test_loader, DEVICE)
        print(f'Test Accuracy: {test_accuracy.item()*100:.2f}%')

    # --------------------------------------------------------------
    # ëª¨ë¸ ê²°ê³¼ ì‹œê°í™” (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ìƒ˜í”Œ í™•ì¸)
    visualize_model_results(model, test_loader, DEVICE, num_samples=1)
    visualize_model_results_with_plot(model, test_loader, DEVICE, num_samples=1)

if __name__ == '__main__':
    main()
